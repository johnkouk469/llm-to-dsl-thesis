Declarative Approach for Building Virtual Assistants
In this section, we present the concepts of our meta-model and its grammar, the implemented processes for model validation and code generation, and finally, the cloud architecture for Virtual Assistant (VA) development and testing. TextX has been employed for meta-model definition and grammar development, while the VA models are transformed into ready-to-deploy Rasa models.

3.1 Modeling with dFlow
A task-oriented Virtual Assistant incorporates the following two components: the Natural Language Understanding (NLU) component, which is responsible for processing user utterances and interpreting the user’s goals or intents, and the Natural Language Generation (NLG) part, which is responsible for creating the most appropriate responses and actions. The root meta-model of dFlow consists of six concepts as depicted in Figure 1: Entity, Synonym, Trigger, EService, Global Slot, and Dialogues. The first three concepts capture the NLU part of the assistant, the next two define reusable features in general scope, and the last one describes the dialogue flows and the assistant responses, encompassing the entire NLG component. A dFlow model incorporates these concepts at the root scope and can be utilized to define the interactive part and include bot responses to predefined conditions (e.g., an internal intent is triggered). All these are discussed next.

3.1.1 Entities & Synonyms
Entities are structured pieces of information inside a user message that can be extracted and used by the assistant. They can be real-world objects or meanings, such as a person, a location, an organization, or a product. DFlow can employ pre-trained Named Entity Recognition (NER) models that can efficiently extract those types of entities without further training. Frequently, Virtual Assistants need to detect use-case-specific information not supported by pre-trained NER models, such as types of food or fruits. In this case, Trainable Entities can be specified and trained during deployment given a set of entity examples. The Entity meta-model is presented in Figure 2.

On the other hand, Synonyms map words to a value other than the literal text extracted. They can be used when users refer to the same thing in multiple ways without semantic difference.

3.1.2 Triggers
Triggers represent the two ways a dialogue can be initiated: when a user states a particular expression or Intent, or when an external Event is triggered, such as a reminder or a notification. In task-based dialogue systems, an Intent is a goal the user is trying to achieve or accomplish, such as retrieving specific information on the weather or setting a reminder. An Intent requires a set of phrase examples that are semantically similar to the expected user expressions. These examples can consist of combinations of text, pre-trained and trainable entities, as well as synonyms, as shown in Figure 3. Events are system-initiated and do not need any phrase examples.

3.1.3 EServices
External services are REST endpoints that can be used as part of the VA’s responses. Their URL and HTTP method are defined globally as static attributes, while their parameters can be specified inside the dialogue section when called, in a more dynamic manner as depicted in Figure 4.

3.1.4 Global Slots
Slots are static information an assistant can access and use, offering multi-turn conversations, memory, and personalization. DFlow introduces the GSlot concept to define variables in the global scope so they can be accessed by various Dialogues, Forms, and Actions, as discussed in the following paragraphs.

3.1.5 Dialogues
An important concept of the dFlow meta-model is the Dialogue. Dialogues are conversational flows the assistant supports in the form of trigger and response pairs, where each response is a sequence of Forms and ActionGroups in a one-turn conversation manner. Each trigger initiates only one dialogue since more complex scenarios should consider the conversation history, which is out of the scope of the current dFlow version as presented in Figure 5.

Regarding the responses, a Form is a conversational pattern to collect information and store it in form parameters or form slots, following business logic. Two interaction methods are supported by the dFlow DSL: Human-Robot Interaction (HRI) and External Services. Information can be collected via HRI, where the assistant sequentially collects the information from the user by requesting each slot using specified text and extracting data from the user expression as presented in Figure 6. The expression can contain the entire text, an extracted entity, or a specific value set mapped to a particular intent. The second choice is the EServiceSource interaction, where the slot is filled with information received from an external service, previously defined as an EService in the dFlow model.

Furthermore, an ActionGroup is a set of Actions. The dFlow language supports five different types of actions: the assistant can state a given phrase (SpeakAction), fire a broker event (FireEventAction), call a REST endpoint (RESTCallAction), set a global slot (SetGSlot) or form slot (SetFSlot) with particular parameters. Actions can also use real-time environment parameters and functions grouped as UserProperties and SystemProperties. User properties are user information stored locally on the device that the assistant can use, such as the name, surname, age, email, phone, city, and address. System properties are in-built system functions to get the current time, location, and a random integer or float. This allows the assistant to access data while being device-agnostic and offering more dynamic and personalized dialogues.

3.2 Software Automation
A Model-to-Text (M2T) transformation has been developed to enable the automated generation of ready-to-deploy virtual assistants. The M2T takes a dFlow model as input and uses the Rasa framework to build the assistant and integrate dialogue flows defined by the user. The M2T transformation processes and maps each dFlow attribute to one or more corresponding Rasa attributes, as the Rasa file structure consists of several configuration files and Python scripts for static and dynamic behavior, respectively. For example, a simple hello-world one-dialogue VA implemented on Rasa would need eight different files and a total of 52 lines of code, while it would require 16 lines of dFlow as presented in Model 1.

Furthermore, the dFlow DSL implements a model validator, used to validate dFlow instance models against various relational and logical rules defined by the meta-model. Model validation can be executed both at development time, like any other language syntax checker, and before performing the M2T. The validation process minimizes errors beyond simple syntax issues since domain-specific knowledge is defined by the meta-model that dFlow models must conform to.

3.3 Cloud-native Development and Continuous Delivery
All systems have been transferred to the cloud for location-agnostic development and deployment of Virtual Assistants by both citizen and software developers. This cloud-native approach comprises several components as depicted in Figure 7. The core ones are the REST API of dFlow and a Rasa deployment that includes several services (action server, database, event broker, telemetry, Rasa API for remote interaction, lock store, and tracker store). Everything is deployed within the cluster, and GitOps via Github Actions (GHA) is used to automate the build and deployment of Rasa instances within the cluster. Additionally, a Discord bot connecting user interfaces to the aforementioned backend services has also been deployed in the cluster.

Basic dFlow operations (e.g., validation and code generation) have been RESTified to provide a number of endpoints in the form of an API based on the OpenAPI specification. A model can be validated against the domain meta-model and its constraints, generated into a ready-to-deploy VA via an M2T transformation, retrieved, stored, or updated while keeping the older models. The API also provides an endpoint to retrieve the last stored model of a user, and an endpoint that enables merging the last submitted models from all registered users into a single model that includes all dialogue flows defined within the individual dFlow models. Model merging can be used in collaborative development schemes for multi-user VAs. An SQL database has been integrated into the API backend for storage and retrieval of instance models submitted by registered users, as well as for storing user accounts and for performing authentication and authorization for the endpoints. Table 1 documents the essential endpoints that compose the dFlow REST API.

Next, a Rasa VA stack has been deployed within a Kubernetes cluster. Rasa provides several in-built REST endpoints, three of which are utilized to perform operations and interact with VA instances: the Train endpoint to receive a complete Rasa model and perform the internal training process, the model activation endpoint to automatically deploy the latest trained model, and the Dialogue endpoint to chat with a running VA instance. Rasa also employs an optional custom Action Server, a Docker image containing Python scripts responsible for dynamic assistant responses. As all dFlow models generate such responses, it is mandatory for the deployment of our models. Automated continuous integration and continuous delivery processes have been implemented using Github Actions for automated build and upload of software artifacts, and ArgoCD, a declarative continuous delivery tool for cloud-based applications, for automatically uploading the deployments triggered from successful execution of the GHA beforehand.

Finally, the most important component is the interface that allows users to interact with dFlow, validate, generate, and deploy VAs. Discord has been selected as a simple messaging platform where both development and testing can occur. The DiscordPy library has been employed for the implementation of the dFlow discord bot, which supports several keyword-triggered commands for accessing the aforementioned REST API of dFlow. If no command is detected, it is assumed that the input message is an actual conversation and is routed to the activated Rasa model for direct interaction with the previously deployed VA.

3.4 Development and Deployment Process Demonstration
The entire cloud-native architecture can be better demonstrated with the following example. A Virtual Assistant has been designed that supports three scenarios: greeting the user, finding the nearest open pharmacies, and telling jokes. These scenarios require three intents (Model 2), three external services (Model 3), and three dialogues (Model 4). The dFlow model, which is the concatenation of the three presented models, is uploaded to the Discord platform for validation to check its syntax. Upon successful validation, it is sent for generation and deployment. This interaction is also depicted in Figure 8. The dFlow model consists of one file of 72 lines of code, while the generated Rasa model consists of eight different files with a total of 357 lines of code. After a few minutes, the Discord bot notifies us that the deployment of the generated model to the Rasa cluster is completed, allowing us to interact with it and conduct the conversation presented in Figure 9.

Model 2 Demonstration assistant Triggers:

```dflow
triggers
Intent greet
"hey",
"hello there",
"good morning",
"good afternoon",
"what’s up"
end
Intent find_pharmacy
 "which pharmacy is open today",
 "I need to go to the pharmacy right now",
 "I have to buy new medicine",
 "open pharmacies"
 end
 Intent tell_joke
 "tell me a joke",
 "do you know any good jokes",
 "time for a laugh",
 "make me laugh"
 end
 end
```

Model 3 Demonstration assistant Eservices:

```dflow
eservices
EServiceHTTP coords_svc
verb: GET
host: ’http://services.issel.ee.auth.gr’
path: ’/geospatial/get_coords’
end
EServiceHTTP pharmacy_svc
verb: GET
host: ’http://services.issel.ee.auth.gr’
 path: ’/medical/pharmacies_nearest’
 end
 EServiceHTTP jokes_svc
 verb: GET
 host: ’http://services.issel.ee.auth.gr’
 path: ’/quotes/get_joke’
 end
 end
```